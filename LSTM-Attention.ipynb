{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import exp\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = quandl.get(\"USTREASURY/YIELD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################\n",
    "def clean_data(df):\n",
    "    df = df[[\"2 YR\", \"10 YR\", \"20 YR\"]]\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def normalize_data(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    for column in df.columns:\n",
    "        scaler.fit(df[column].values.reshape(-1,1))\n",
    "        df[column] = scaler.transform(df[column].values.reshape(-1,1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_data(df)\n",
    "df = normalize_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"B(0)\"] = None\n",
    "df[\"B(1)\"] = None\n",
    "df[\"B(2)\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(lamb,t):\n",
    "    a = 1\n",
    "    b = (1 -(exp(-t*lamb)))/ (t * lamb)\n",
    "    c = b - exp(-t * lamb)\n",
    "    return a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(df):\n",
    "    \n",
    "    lamb = 0.0609 # Diebold-Li\n",
    "    \n",
    "    a2, b2, c2 = get_params(lamb, 2)\n",
    "    a10, b10, c10 = get_params(lamb, 10)\n",
    "    a20, b20, c20 = get_params(lamb, 20)\n",
    "    \n",
    "    for row in range(len(df)):\n",
    "        y2 = df[\"2 YR\"].iloc[row]\n",
    "        y10 = df[\"10 YR\"].iloc[row]\n",
    "        y20 = df[\"20 YR\"].iloc[row]\n",
    "        \n",
    "        x2 = [a2, b2, c2]\n",
    "        x10 = [a10, b10, c10]\n",
    "        x20 = [a20, b20, c20]\n",
    "        \n",
    "        X = [x2, x10, x20]\n",
    "        X = sm.add_constant(X)\n",
    "        \n",
    "        y = [y2, y10, y20]\n",
    "        \n",
    "        model = sm.OLS(y, X).fit()\n",
    "        \n",
    "        df[\"B(0)\"].iloc[row], df[\"B(1)\"].iloc[row], df[\"B(2)\"].iloc[row] = model.params\n",
    "\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_params(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (26,11))\n",
    "plt.plot(df[\"B(0)\"], color = \"r\", label = \"beta_0\")\n",
    "plt.plot(df[\"B(1)\"], color = \"g\", label = \"beta_1\")\n",
    "plt.plot(df[\"B(2)\"], color = \"b\", label = \"beta_2\")\n",
    "plt.legend(loc = 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Activation, Dropout, RepeatVector, Concatenate, Reshape, Input, Dot\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from tensorflow.nn import tanh, softmax\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(df.values[:,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[:4000]\n",
    "test = data[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_activation(x):\n",
    "    e = K.exp(x - K.max(x, axis=1, keepdims=True))\n",
    "    s = K.sum(e, axis=1, keepdims=True)\n",
    "    return e / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Attention(object):\n",
    "    \n",
    "    def __init__(self, X, Y, n_h1, n_h2,  attn_neurons, epochs,\n",
    "                 batch_size ,\n",
    "                 predict ,\n",
    "                 lookback,\n",
    "                 attn_sharing = True,\n",
    "                 regularization = (0.00001, \"0.00001\"),\n",
    "                 ):\n",
    "        \n",
    "        keras.clear_session()\n",
    "        tf.reset_default_graph()\n",
    "        self.learning(True)\n",
    "        \n",
    "                \n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.attn_sharing = attn_sharing\n",
    "        \n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.n_h1 = n_h1\n",
    "        self.n_h2 = n_h2\n",
    "        #self.n_h3 = n_h3\n",
    "        self.attn_neurons = attn_neurons\n",
    "        \n",
    "        self.n_obs = self.X.shape[0]\n",
    "        self.len_input = self.X.shape[1]\n",
    "        self.dim_input = self.X.shape[2]\n",
    "        self.n_output = self.y.shape[1]\n",
    "        \n",
    "        self.regularization = regularization[0]\n",
    "        \n",
    "        \n",
    "        assert self.n_input == self.n_output\n",
    "        \n",
    "        self.attn_activation = \"selu\"\n",
    "        self.attn_init = \"lecun_normal\"\n",
    "        \n",
    "    def shared_layers(self):\n",
    "        if self.regularization > 0:\n",
    "            self.kernel_regularizer = regularizers.l2(self.regularization)\n",
    "            self.LSTM_regularizer = regularizers.l2(self.regularization)\n",
    "            self.dropout = 0.2\n",
    "            self.bias_regularizer = regularizers.l2(self.regularizers)\n",
    "        else:\n",
    "            self.kernel_regularizer = self.bias_regularization = self.LSTM_regularization = None\n",
    "            self.dropout = 0.2\n",
    "            \n",
    "        if self.attn_sharing:\n",
    "            if self.attn_neurons > 0:\n",
    "                self.attn_h01 = Dense(self.attn_neurons,\n",
    "                                     kernel_regularizer = self.kernel_regularizer,\n",
    "                                     activation = self.attn_activation,\n",
    "                                     kernel_initializer = self.attn_init)\n",
    "            \n",
    "            self.attn_h1 = Dense(1,\n",
    "                                     kernel_regularizer = self.kernel_regularizer,\n",
    "                                     activation = self.attn_activation,\n",
    "                                     kernel_initializer = self.attn_init)\n",
    "            \n",
    "        self.h2 = LSTM(self.n_h2,\n",
    "                      kernel_regularizer = self.kernel_regularizer,\n",
    "                                     bias_regularizer = self.bias_reularizer,\n",
    "                       recurrent_regularizer = self.LSTM_regularizer,\n",
    "                       recurrent_dropout = self.dropout,\n",
    "                       return_state = True \n",
    "                                     )\n",
    "        \n",
    "        self.Output = Dense( 1,\n",
    "                            kernel_regularizer = self.kernel_regularizer,\n",
    "                                     bias_regularizer = self.bias_reularizer,\n",
    "                            activation = \"linear\"\n",
    "                           )\n",
    "        \n",
    "        def LSTMAttention(self):\n",
    "            self.shared_layers()\n",
    "            \n",
    "            inputs = Input(shape = (self.len_input, self.dim_input))\n",
    "             \n",
    "            X = LSTM(self.n_h1,\n",
    "                     kernel_regularizer = self.kernel_regularizer,\n",
    "                                     bias_regularizer = self.bias_reularizer,\n",
    "                       recurrent_regularizer = self.LSTM_regularizer,\n",
    "                       recurrent_dropout = self.dropout,\n",
    "                       return_sequences = True \n",
    "                    )(inputs)\n",
    "            \n",
    "            X = Reshape((self.len_input , self.n_h2))(X)\n",
    "            \n",
    "            h_t = Input(shape = (self.n_h2))\n",
    "            c_t = Input(shape = (self.n_h2))\n",
    "            h_t1 = h_t\n",
    "            c_t1 = c_t\n",
    "            \n",
    "            preds = list()\n",
    "            \n",
    "            for i in range(self.n_outputs + 1):\n",
    "                h_t1_repeat = RepeatVector(self.len_input)(h_t1)\n",
    "                joined = Concatenate(axis = -1)([X, h_t1_repeat])\n",
    "                \n",
    "                if self.attn_neurons>0:\n",
    "                    if self.attn_sharing:\n",
    "                        joined = self.attn_h01(joined)\n",
    "                    else:\n",
    "                        joined = Dense(self.attn_neurons,\n",
    "                                      kernel_regularizer = self.kernel_regularizer,\n",
    "                                     activation = self.attn_activation,\n",
    "                                     kernel_initializer = self.attn_init)(joined)\n",
    "                \n",
    "                if self.attn_sharing:\n",
    "                    e_vals = self.attn_h1(joined)\n",
    "                else:\n",
    "                    e_vals = Dense( 1,\n",
    "                                   kernel_regularizer = self.kernel_regularizer,\n",
    "                                     activation = self.attn_activation,\n",
    "                                     kernel_initializer = self.attn_init\n",
    "                                  )(joined)\n",
    "                \n",
    "                \n",
    "                alphas = Activation(softmax_activation)(e_vals)\n",
    "                attns = Dot(axes = 1)([alphas , X])\n",
    "                \n",
    "                h_t1 , _, c_t1 = self.h2(attns, initial_state = [h_t1, c_t1])\n",
    "                \n",
    "                if i>0:\n",
    "                    pred = self.Output(h_t1)\n",
    "                    preds.append(pred)\n",
    "            \n",
    "            self.model = Model(inputs = [inputs , h_t, c_t], outputs = preds)\n",
    "            self.model.compile(loss = \"mse\", optimizer =\"adam\", metrics =[\"mse\"])\n",
    "            \n",
    "            print(model.summary())\n",
    "            \n",
    "        def fit(self):\n",
    "            self.learning(True)\n",
    "            \n",
    "            h_t = np.zeros((self.n_obs , self.n_h2))\n",
    "            c_t = np.zeros((self.n_obs, self.n_h2))\n",
    "            \n",
    "            y_split = np.split(self.Y, indices_or_sections = self.n_outputs, axis = 1)\n",
    "        \n",
    "            self.model.fit([self.X, h_t, c_t],\n",
    "                          y_split,\n",
    "                          epochs = self.epochs,\n",
    "                          batch_size = self.batch_size,\n",
    "                          shuffle = True,\n",
    "                          verbose = 2,\n",
    "                          validation_split = 0.3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "N_Attention_shared = 0\n",
    "\n",
    "# n_input = 1\n",
    "# n_output = 1\n",
    "\n",
    "n_h1 = 20\n",
    "n_h2 = 20\n",
    "\n",
    "batch_size = 32\n",
    "lookback = 30\n",
    "predict = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = LSTM_Attention(X, Y, n_h1, n_h2,  N_Attention_shared, N_EPOCHS,\n",
    "                 batch_size ,\n",
    "                 predict ,\n",
    "                 lookback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTM_Attention(tf.keras.Model):\n",
    "#     def __init__( self , n_input, n_output, n_hidden,  batch_size,  time_step):\n",
    "        \n",
    "#         super(LSTM_Attention, self).__init__()\n",
    "#         self.n_input = n_input\n",
    "#         self.n_output = n_hidden\n",
    "#         self.neurons = n_hidden\n",
    "#         self.batch_size = batch_size\n",
    "#         self.time_step = time_step\n",
    "        \n",
    "#         self.lstm_layer = LSTM(self.neurons,\n",
    "#                               return_sequences = True,\n",
    "#                               return_state = True,\n",
    "#                               stateful = True,\n",
    "#                               recurrent_initializer = \"glorot_uniform\")\n",
    "        \n",
    "#         self.Wh = Dense(self.neurons)\n",
    "#         self.Ws = Dense(self.neurons)\n",
    "#         self.Wx = Dense(1)\n",
    "#         self.V = Dense(1)\n",
    "#         self.O = Dense(self.n_output)\n",
    "\n",
    "#     def get(self, X, hidden, s_t):\n",
    "#         hidden = tf.expand_dims(hidden, 1)\n",
    "#         X = tf.expand_dims(X, 0)\n",
    "#         atten = self.V(tanh( self.Wx(X) + \n",
    "#                                   self.Wh(hidden) + \n",
    "#                                  self.Ws(s_t)))\n",
    "#         weights = softmax(atten, axis = 1)\n",
    "#         o1, s_h, s_c = self.lstm(X * weights)\n",
    "#         output = self.O(s_h)\n",
    "        \n",
    "#         return weights, s_h, s_c, output\n",
    "    \n",
    "#     def zero_init(self):\n",
    "#         hidden = tf.zeros((self.batch_size, self.neurons))\n",
    "#         s_t = tf.zeros((self.batch_size, self.time_step, self.neurons))\n",
    "        \n",
    "#         return hidden, s_t\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_train = tf.data.Dataset.from_tensor_slices(train_data[-250:]).window(size=time_step+shift, shift=shift, drop_remainder=True).flat_map(\n",
    "#         lambda x: x.batch(time_step+shift))\n",
    "\n",
    "# tdata = tf_train.map(lambda x: tf.reshape(x[:time_step],[time_step,1]))\n",
    "# labels = tf_train.map(lambda x: x[-shift:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(inp, target, hidden,s_t):\n",
    "#     loss = 0\n",
    "#     pred = 0\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         for i,t in zip(inp,target):\n",
    "#             weight, hidden, s_t, output = attention(tf.cast(i, tf.float32), hidden, s_t)\n",
    "#             loss += tf.keras.losses.mean_squared_error(t, output)\n",
    "#             pred += 1\n",
    "#     variables = attention.trainable_variables\n",
    "#     grads = tape.gradient(loss,variables)\n",
    "#     optimizer.apply_gradients(zip(grads, variables))\n",
    "#     return loss/pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses = []\n",
    "# for epoch in range(N_EPOCHS):\n",
    "#     hidden, s_t = attention.zero_init()\n",
    "#     loss = train()\n",
    "    \n",
    "#     Losses.append(loss)\n",
    "    \n",
    "#     if epoch % 2 == 0:\n",
    "#         print(\"Epoch : {}, Loss : {}\".format(epoch+1, loss.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
